{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 處理特殊狀況的句子\n",
    "def Filter_Title(sen):\n",
    "    flag = False\n",
    "    doc = nlp(sen)\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            flag = True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flair NER Model\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('ner-ontonotes')\n",
    "def sentence_ner(sen):\n",
    "    sentence = Sentence(sen, use_tokenizer=True)\n",
    "    tagger.predict(sentence)\n",
    "    ner_tmp ,ner_words,ner_tags = [],[],[]\n",
    "    for token in sentence:\n",
    "        if token.get_tag('ner').find('ORG')>=0:\n",
    "            ner_words.append(token.text)\n",
    "            ner_tags.append(token.get_tag('ner'))\n",
    "    index = 0\n",
    "    flag = False\n",
    "    flag_i = False\n",
    "    for word , tag in zip(ner_words , ner_tags):\n",
    "        if  tag =='B-ORG':\n",
    "            flag = True\n",
    "            ner_tmp.append(word)\n",
    "        if flag:\n",
    "            if 'I-ORG' == tag:\n",
    "                ner_tmp[index] = ner_tmp[index] +' '+ word\n",
    "                flag_i = True         \n",
    "        if flag_i:\n",
    "            if 'E-ORG' == tag:\n",
    "                ner_tmp[index] = ner_tmp[index] +' '+ word\n",
    "                flag , flag_i = False , False\n",
    "                index += 1     \n",
    "        if tag == 'S-ORG':\n",
    "            ner_tmp.append(word)\n",
    "            index+=1\n",
    "    return ner_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK句子切割\n",
    "def nltk_sentence_token(Content):\n",
    "    Sen= []\n",
    "    sen_token = sent_tokenize(Content)\n",
    "    for sen in sen_token:\n",
    "        if len(sen.split(' '))>=5 and Filter_Title(sen):\n",
    "            Sen.append(sen)\n",
    "    return Sen\n",
    "## 前處理(過濾表格、圖、標題)\n",
    "def Deal_html_return_sen(html_list):\n",
    "    Sen = []\n",
    "    for path_html in html_list:\n",
    "        file = open(path_html , 'r')\n",
    "        soup = BeautifulSoup(file , 'html.parser')\n",
    "        file.close()\n",
    "        tmpp = soup.find_all('p')\n",
    "        After_filter_title_sen = [i.text.strip() for i in tmpp if i.text and len(i.text.split(' '))>=6 and Filter_Title(i.text)]\n",
    "        Content = ' '.join(After_filter_title_sen)\n",
    "        Sen+=nltk_sentence_token(Content)\n",
    "    return Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 讀取黑白名單\n",
    "def Load_WhilteBlack(path):\n",
    "    b_list = []\n",
    "    w_list = []\n",
    "    df_e = pd.read_csv(path)\n",
    "    for e , label in zip(df_e['Entity'] , df_e['islabel']):\n",
    "        e=e.strip().replace('\\ue5b8','')\n",
    "        label = str(label)\n",
    "        if label=='1':\n",
    "            w_list.append(e)\n",
    "        else:\n",
    "            b_list.append(e) \n",
    "    w_list = list(set(w_list))\n",
    "    b_list = list(set(b_list))\n",
    "    return (w_list , b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句子用黑白名單 + Flair Model後，生出來的新的NER名單(人要去Check)\n",
    "def Find_NER_In_Sentence_Flair(Total_sen , w_list, b_list):\n",
    "    from flashtext import KeywordProcessor\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    for i in w_list:keyword_processor.add_keyword(i)\n",
    "    Entity = []\n",
    "    for s in Total_sen:\n",
    "        Flair = sentence_ner(s)\n",
    "        w = keyword_processor.extract_keywords(s)\n",
    "        total = list(set(w+Flair))\n",
    "        result = []\n",
    "        for i in total:\n",
    "            i=i.replace('(','').replace(')','').replace('\"','').strip()\n",
    "            if i in b_list:\n",
    "                pass\n",
    "            if i.split(' ')[0].lower()=='the':\n",
    "                pass\n",
    "            else:\n",
    "                result.append(i)\n",
    "        result = list(set(result))\n",
    "        Entity+=result\n",
    "    Entity = list(set(Entity))\n",
    "    Entity = [e for e in Entity if e[0].istitle() ]\n",
    "    T_E = []\n",
    "    for i in Entity:\n",
    "        try:\n",
    "            if i not in b_list:\n",
    "                if i[-2]==' ':\n",
    "                    i=i.rstrip('.').strip()+'.'\n",
    "                    T_E.append(i)\n",
    "                else:\n",
    "                    T_E.append(i)\n",
    "            else:pass\n",
    "        except:\n",
    "            pass\n",
    "    Ent = list(set(T_E))\n",
    "    return Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Use_whilte_check_flair(Ent,w_list):\n",
    "    Tmp_e = []\n",
    "    bin_go=[]\n",
    "    for e in Ent:\n",
    "        if e in w_list:\n",
    "            bin_go.append('O')\n",
    "        else:\n",
    "            bin_go.append('X')\n",
    "        Tmp_e.append(e)\n",
    "    E = [(b , e)for b , e in zip(Tmp_e , bin_go)]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = glob.glob('xxxx/*/html/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_sen = Deal_html_return_sen(html_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42537\n"
     ]
    }
   ],
   "source": [
    "print(len(Total_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list , b_list = Load_WhilteBlack('flair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity_tmp = Find_NER_In_Sentence_Flair(Total_sen,w_list,b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final_Entity = use_black_list(Entity_tmp , b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Use_whilte_check_flair(Final_Entity , w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('123.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([('Entity','white_list')])\n",
    "    writer.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
