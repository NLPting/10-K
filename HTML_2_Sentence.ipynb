{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "import sys\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "keyword_processor = KeywordProcessor(case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 處理特殊狀況的句子\n",
    "def Filter_Title(sen):\n",
    "    flag = False\n",
    "    doc = nlp(sen)\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "## 前處理(過濾表格、圖、標題)\n",
    "def Deal_html_return_sen(html_list):\n",
    "    ## NLTK句子切割\n",
    "    def nltk_sentence_token(Content):\n",
    "        Sen= []\n",
    "        sen_token = sent_tokenize(Content)\n",
    "        for sen in sen_token:\n",
    "            if len(sen.split(' '))>=5 and Filter_Title(sen):\n",
    "                Sen.append(sen)\n",
    "        return Sen\n",
    "    ## 處理特殊狀況的句子\n",
    "    def Filter_Title(sen):\n",
    "        flag = False\n",
    "        doc = nlp(sen)\n",
    "        for token in doc:\n",
    "            if token.pos_=='VERB':\n",
    "                flag = True\n",
    "        return flag\n",
    "    Sen = []\n",
    "    for path_html in html_list:\n",
    "        file = open(path_html , 'r')\n",
    "        soup = BeautifulSoup(file , 'html.parser')\n",
    "        file.close()\n",
    "        tmpp = soup.find_all('p')\n",
    "        After_filter_title_sen = [i.text.strip() for i in tmpp if i.text and len(i.text.split(' '))>=6 and Filter_Title(i.text)]\n",
    "        Content = ' '.join(After_filter_title_sen)\n",
    "        Sen+=nltk_sentence_token(Content)\n",
    "    return Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.read_csv('flair.csv')\n",
    "df_e[:1]\n",
    "Ent = []\n",
    "for ent , label in zip(df_e['Entity'],df_e['islabel']):\n",
    "    if label == 1:\n",
    "        Ent.append(ent)\n",
    "Ent = [i.replace('LLC','').replace('Ltd','').replace('Inc','').replace(',','').replace('N.V','').replace('Co.','').replace('.','').strip() for i in Ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Ent:keyword_processor.add_keyword(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxxx/Broadcom/html/2014.html', 'xxxx/Broadcom/html/2014-2.html']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_list = glob.glob('xxxx/Broadcom/html/2014*.html')\n",
    "html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_sen = Deal_html_return_sen(html_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcom\n",
      "2014\n"
     ]
    }
   ],
   "source": [
    "Main_Company = html_list[0].split('/')[1]\n",
    "Year = html_list[0].split('/')[-1].split('-')[0].replace('.html','')\n",
    "print(Main_Company)\n",
    "print(Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmp = []\n",
    "for s in Total_sen:\n",
    "    Ent = keyword_processor.extract_keywords(s)\n",
    "    for e in Ent:\n",
    "        Tmp.append((s , Main_Company,e , Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test.sentence.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([('Sentence','Main','Secondary','Time','Label')])\n",
    "    writer.writerows(Tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12715"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
