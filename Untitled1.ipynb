{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 處理特殊狀況的句子\n",
    "def Filter_Title(sen):\n",
    "    flag = False\n",
    "    doc = nlp(sen)\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            flag = True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flair NER Model\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "def sentence_ner(sen):\n",
    "    sentence = Sentence(sen, use_tokenizer=True)\n",
    "    tagger.predict(sentence)\n",
    "    ner_tmp ,ner_words,ner_tags = [],[],[]\n",
    "    for token in sentence:\n",
    "        if token.get_tag('ner').find('ORG')>=0:\n",
    "            ner_words.append(token.text)\n",
    "            ner_tags.append(token.get_tag('ner'))\n",
    "    index = 0\n",
    "    flag = False\n",
    "    flag_i = False\n",
    "    for word , tag in zip(ner_words , ner_tags):\n",
    "        if  tag =='B-ORG':\n",
    "            flag = True\n",
    "            ner_tmp.append(word)\n",
    "        if flag:\n",
    "            if 'I-ORG' == tag:\n",
    "                ner_tmp[index] = ner_tmp[index] +' '+ word\n",
    "                flag_i = True         \n",
    "        if flag_i:\n",
    "            if 'E-ORG' == tag:\n",
    "                ner_tmp[index] = ner_tmp[index] +' '+ word\n",
    "                flag , flag_i = False , False\n",
    "                index += 1     \n",
    "        if tag == 'S-ORG':\n",
    "            ner_tmp.append(word)\n",
    "            index+=1\n",
    "    return ner_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK句子切割\n",
    "def nltk_sentence_token(Content):\n",
    "    Sen= []\n",
    "    sen_token = sent_tokenize(Content)\n",
    "    for sen in sen_token:\n",
    "        if len(sen.split(' '))>=5 and Filter_Title(sen):\n",
    "            Sen.append(sen)\n",
    "    return Sen\n",
    "## 前處理(過濾表格、圖、標題)\n",
    "def Deal_html_return_sen(html_list):\n",
    "    Sen = []\n",
    "    for path_html in html_list:\n",
    "        file = open(path_html , 'r')\n",
    "        soup = BeautifulSoup(file , 'html.parser')\n",
    "        file.close()\n",
    "        tmpp = soup.find_all('p')\n",
    "        After_filter_title_sen = [i.text.strip() for i in tmpp if i.text and len(i.text.split(' '))>=6 and Filter_Title(i.text)]\n",
    "        Content = ' '.join(After_filter_title_sen)\n",
    "        Sen+=nltk_sentence_token(Content)\n",
    "    return Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 讀取黑白名單\n",
    "def Load_WhilteBlack(path):\n",
    "    b_list = []\n",
    "    w_list = []\n",
    "    df_e = pd.read_csv(path)\n",
    "    for e , label in zip(df_e['Entity'] , df_e['islabel']):\n",
    "        e=e.strip().replace('\\ue5b8','')\n",
    "        label = str(label)\n",
    "        if label=='1':\n",
    "            w_list.append(e)\n",
    "        else:\n",
    "            b_list.append(e) \n",
    "    w_list = list(set(w_list))\n",
    "    b_list = list(set(b_list))\n",
    "    return (w_list , b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句子用黑白名單 + Flair Model後，生出來的新的NER名單(人要去Check)\n",
    "def Find_NER_In_Sentence_Flair(Total_sen , w_list, b_list):\n",
    "    from flashtext import KeywordProcessor\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    for i in w_list:keyword_processor.add_keyword(i)\n",
    "    Entity = []\n",
    "    for s in Total_sen:\n",
    "        print(s)\n",
    "        Flair = sentence_ner(s)\n",
    "        w = keyword_processor.extract_keywords(s)\n",
    "        total = list(set(w+Flair))\n",
    "        result = []\n",
    "        for i in total:\n",
    "            i=i.replace('(','').replace(')','').replace('\"','').strip()\n",
    "            if i in b_list:\n",
    "                pass\n",
    "            if i.split(' ')[0].lower()=='the':\n",
    "                pass\n",
    "            else:\n",
    "                result.append(i)\n",
    "        result = list(set(result))\n",
    "        print(result)\n",
    "        print('-------')\n",
    "        Entity+=result\n",
    "    Entity = list(set(Entity))\n",
    "    Entity = [e for e in Entity if e[0].istitle() ]\n",
    "    T_E = []\n",
    "    for i in Entity:\n",
    "        try:\n",
    "            if i not in b_list:\n",
    "                if i[-2]==' ':\n",
    "                    i=i.rstrip('.').strip()+'.'\n",
    "                    T_E.append(i)\n",
    "                else:\n",
    "                    T_E.append(i)\n",
    "            else:pass\n",
    "        except:\n",
    "            pass\n",
    "    Ent = list(set(T_E))\n",
    "    return Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Use_whilte_check_flair(Ent,w_list):\n",
    "    Tmp_e = []\n",
    "    bin_go=[]\n",
    "    for e in Ent:\n",
    "        if e in w_list:\n",
    "            bin_go.append('O')\n",
    "        else:\n",
    "            bin_go.append('X')\n",
    "        Tmp_e.append(e)\n",
    "    E = [(b , e)for b , e in zip(Tmp_e , bin_go)]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxxx/AMD/html/2012-1.html']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_list = glob.glob('xxxx/AMD/html/2012*.html')[1:2]\n",
    "html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_sen = Deal_html_return_sen(html_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list , b_list = Load_WhilteBlack('flair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statements in this report include forward-looking statements.\n",
      "[]\n",
      "-------\n",
      "These forward-looking statements are based on current expectations and beliefs and involve numerous risks and uncertainties that could cause actual results to differ materially from expectations.\n",
      "[]\n",
      "-------\n",
      "These forward- looking statements should not be relied upon as predictions of future events as we cannot assure you that the events or circumstances reflected in these statements will be achieved or will occur.\n",
      "['Future']\n",
      "-------\n",
      "You can identify forward-looking statements by the use of forward-looking terminology including “believes,” “expects,” “may,” “will,” “should,” “seeks,” “intends,” “plans,” “pro forma,” “estimates,” or “anticipates” or the negative of these words and phrases or other variations of these words and phrases or comparable terminology.\n",
      "[]\n",
      "-------\n",
      "The forward-looking statements relate to, among other things:demand for our products; the timing of new product releases and technology transitions; the growth and competitive landscape of the markets in which we participate; the nature and extent of our future payments to GLOBALFOUNDRIES Inc. (GF) under the wafer supply agreement (WSA) and the materiality of these payments; manufacturing yields at GF and constrained supply of products from GF; the 2011 restructuring plan, including the timing of actions in connection with the plan and anticipated restructuring charges, cash expenditures, operational savings, and our intention to use these savings to fund certain strategic initiatives; the level of international sales as compared to total sales; the availability of external financing; our ability to sell our auction rate securities in the next twelve months; that our cash, cash equivalents and marketable securities and anticipated cash flow from operations and available external financing will be sufficient to fund our operations over the next twelve months; our dependence on a small number of customers; our hedging strategy; the continued shortage of hard disk drives as a result of the floods in Thailand; and the adequacy of our existing facilities for our present purpose.\n"
     ]
    }
   ],
   "source": [
    "Entity_tmp = Find_NER_In_Sentence_Flair(Total_sen,w_list,b_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
